{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path_file = \"../../nechkasova-tokenizer/assets/annotated-corpus/train/alt.atheism/49960.tsv\"\n",
    "dataset_path = \"../../nechkasova-tokenizer/assets/annotated-corpus/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lesya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def read_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    return lines\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", '', text)\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    lines = read_data(file_path)\n",
    "    tokens = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.strip():\n",
    "            try:\n",
    "                token, stem, lemma = line.strip().split('\\t')\n",
    "            except:\n",
    "                continue\n",
    "            cleaned_token = clean_text(token)\n",
    "            if cleaned_token and cleaned_token not in stop_words:\n",
    "                tokens.append(cleaned_token)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def process_directory(directory_path):\n",
    "    all_tokens = []\n",
    "\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            # print(f\"Process file {file_path}\")\n",
    "            tokens = process_file(file_path)\n",
    "            all_tokens.append(tokens)\n",
    "\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = process_file(dataset_path_file)\n",
    "tokens = process_directory(dataset_path)\n",
    "# print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=tokens, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(content):\n",
    "    sentence_endings_pattern = re.compile(r'(?<!\\w\\.\\w.)(?<!\\w\\. \\w.)(?<![A-Z][a-z]\\.)(?<!\\s\\.\\s)(?<=\\.|\\?|\\!)\\s(?![A-Z][A-Za-z]\\.)(?!\\w\\. \\w.)|(?<![,])\\n(?![a-zA-Z0-9])')\n",
    "    sentences = sentence_endings_pattern.split(content)\n",
    "\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "    sentences_included_key_value = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        key_value_pattern = re.compile(r'[A-Z](\\w+-)*\\w+(\\s\\w+)*:[ ]*([\\W\\w]+[ ,])+(?!,\\n)')\n",
    "        if key_value_pattern.match(sentence):\n",
    "            add_sentence = re.split(r'(?<!,)\\n', sentence)\n",
    "            sentences_included_key_value.extend(add_sentence)\n",
    "        else:\n",
    "            sentences_included_key_value.append(sentence)\n",
    "    # sentences = [sentence.replace('\\n', ' ') for sentence in sentences]\n",
    "    return sentences_included_key_value\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    pattern = r'\\+?\\d[\\d\\-\\(\\)\\s]{7,}\\d' \\\n",
    "              r'|\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b' \\\n",
    "              r'|\\b(?:Mr|Ms|Mrs|Dr|Prof|St)\\.\\s[A-Z][a-z]+' \\\n",
    "              r'|\\b\\d{1,2}:\\d{2}\\s?(?:[AaPp]\\.?[Mm]\\.?)\\b' \\\n",
    "              r'|\\w+|[^\\w\\s]'\n",
    "    tokens = re.findall(pattern, sentence)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_text(text, model):\n",
    "    sentences = get_sentences(text)\n",
    "    sentence_vectors = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        cleaned_text = clean_text(sentence)\n",
    "        tokens = tokenize_sentence(cleaned_text)\n",
    "        if len(tokens) == 0:\n",
    "            continue\n",
    "        word_vectors = [model.wv[word] if word in model.wv else np.zeros(100) for word in tokens]\n",
    "        sentence_vector = np.mean(word_vectors, axis=0)\n",
    "        sentence_vectors.append(sentence_vector)\n",
    "        \n",
    "    text_vector = np.mean(sentence_vectors, axis=0)\n",
    "    \n",
    "    return text_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[ 0.06603803  0.04041414  0.2965158   0.54403996  0.40739977 -0.44551393\n",
      "  0.1606691   0.5753212  -0.1143528  -0.21388742  0.04817864  0.01289042\n",
      " -0.13808554  0.02562388 -0.15186119 -0.23477179 -0.20436378 -0.24522659\n",
      "  0.05454374 -0.85502017  1.1733248   0.04736743  0.897901    0.12461732\n",
      " -0.5895696  -0.4276418   0.34620756 -0.32519758 -0.5270321   0.41284737\n",
      "  0.21516019 -0.01142962  0.41720778 -0.62410635  0.29935935  0.3833707\n",
      " -0.23130068 -0.6496782  -0.87836826 -0.44881403  0.4692001  -0.6003808\n",
      " -0.74882674 -0.21054474  0.51244307  0.11857323 -0.12278455  0.2909257\n",
      "  0.28487986 -0.4391346   0.02587885 -0.22700348 -0.2843613   0.29164603\n",
      " -1.0175967  -0.18673402 -0.26023012 -0.48501605 -0.3140038  -0.22755022\n",
      "  0.6441806   0.21569479 -0.06850782 -0.23569383 -0.15789339  0.40922463\n",
      " -0.06177444  0.6769148  -0.7453899   0.5271795  -0.11539032  0.7397048\n",
      "  0.03392206 -0.30380297  0.5485276  -0.07419986  0.32635325  0.02989756\n",
      " -1.1561422   0.06447767 -0.41148335 -0.25974905 -0.19926967  0.62558365\n",
      " -0.5572212  -0.37466392 -0.16689274  0.28952226  0.14542085 -0.2554646\n",
      "  0.851077    0.23083141  0.43411824  0.29986697  0.522978    0.7052183\n",
      "  0.2821256  -0.3923141  -0.2802435  -0.01662752]\n"
     ]
    }
   ],
   "source": [
    "text = \"programs applications developing. Sentence\"\n",
    "file_tsv_content = {}\n",
    "\n",
    "vector = vectorize_text(text, model)\n",
    "print(len(vector))\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "programs = applications, developing (programming)\n",
    "interface, buggy, functions, UNIX, code, signals, Internet\n",
    "people, Canada, sport, tatoo\n",
    "\n",
    "\n",
    "email = phone, FAX\n",
    "address, message, contact\n",
    "team, games, baseball\n",
    "\n",
    "lecture = seminar, courses, \n",
    "education, laboratories, conference\n",
    "situation, comments, season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def cosine_similarity(vector_a, vector_b):\n",
    "    return 1 - cosine(vector_a, vector_b)\n",
    "\n",
    "def calculate_distances(word, similar_words, domain_words, different_words, model, cosine_similarity_function):\n",
    "    word_vector = model.wv[word]\n",
    "    distances = []\n",
    "    \n",
    "    for group_name, words in [('Похожие слова', similar_words), \n",
    "                              ('Слова из той же области', domain_words), \n",
    "                              ('Совершенно другие слова', different_words)]:\n",
    "        for w in words:\n",
    "            if w in model.wv:\n",
    "                distance = cosine_similarity_function(word_vector, model.wv[w])\n",
    "                distances.append((group_name, w, distance))\n",
    "    \n",
    "    return sorted(distances, key=lambda x: x[2], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Похожие слова: applications - Косинусное сходство: 0.9429\n",
      "Слова из той же области: interface - Косинусное сходство: 0.8686\n",
      "Слова из той же области: functions - Косинусное сходство: 0.8412\n",
      "Слова из той же области: code - Косинусное сходство: 0.7927\n",
      "Похожие слова: developing - Косинусное сходство: 0.7788\n",
      "Слова из той же области: signals - Косинусное сходство: 0.7311\n",
      "Слова из той же области: buggy - Косинусное сходство: 0.7004\n",
      "Совершенно другие слова: sport - Косинусное сходство: 0.6844\n",
      "Совершенно другие слова: tatoo - Косинусное сходство: 0.5245\n",
      "Совершенно другие слова: people - Косинусное сходство: 0.4397\n"
     ]
    }
   ],
   "source": [
    "similar_words = [\"applications\", \"developing\"]\n",
    "domain_words = [\"interface\", \"buggy\", \"functions\", \"UNIX\", \"code\", \"signals\", \"Internet\"]\n",
    "different_words = [\"people\", \"Canada\", \"sport\", \"tatoo\"]\n",
    "\n",
    "distances = calculate_distances('programs', similar_words, domain_words, different_words, model, cosine_similarity)\n",
    "\n",
    "for group, word, distance in distances:\n",
    "    print(f\"{group}: {word} - Косинусное сходство: {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Похожие слова: address - Косинусное сходство: 0.8983\n",
      "Слова из той же области: contact - Косинусное сходство: 0.8750\n",
      "Слова из той же области: phone - Косинусное сходство: 0.7528\n",
      "Слова из той же области: message - Косинусное сходство: 0.7058\n",
      "Совершенно другие слова: baseball - Косинусное сходство: 0.2184\n",
      "Совершенно другие слова: team - Косинусное сходство: 0.0141\n",
      "Совершенно другие слова: games - Косинусное сходство: 0.0112\n"
     ]
    }
   ],
   "source": [
    "similar_words = [\"address\", \"FAX\"]\n",
    "domain_words = [\"phone\", \"message\", \"contact\"]\n",
    "different_words = [\"team\", \"games\", \"baseball\"]\n",
    "\n",
    "distances = calculate_distances('email', similar_words, domain_words, different_words, model)\n",
    "\n",
    "for group, word, distance in distances:\n",
    "    print(f\"{group}: {word} - Косинусное сходство: {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Похожие слова: seminar - Косинусное сходство: 0.9147\n",
      "Похожие слова: courses - Косинусное сходство: 0.8138\n",
      "Слова из той же области: education - Косинусное сходство: 0.6976\n",
      "Совершенно другие слова: situation - Косинусное сходство: 0.6668\n",
      "Слова из той же области: laboratories - Косинусное сходство: 0.6318\n",
      "Совершенно другие слова: comments - Косинусное сходство: 0.6094\n",
      "Слова из той же области: conference - Косинусное сходство: 0.5666\n",
      "Совершенно другие слова: season - Косинусное сходство: 0.4208\n"
     ]
    }
   ],
   "source": [
    "similar_words = [\"seminar\", \"courses\"]\n",
    "domain_words = [\"education\", \"laboratories\", \"conference\"]\n",
    "different_words = [\"situation\", \"comments\", \"season\"]\n",
    "\n",
    "distances = calculate_distances('lecture', similar_words, domain_words, different_words, model)\n",
    "\n",
    "for group, word, distance in distances:\n",
    "    print(f\"{group}: {word} - Косинусное сходство: {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity_manual(vector_a, vector_b):\n",
    "    dot_product = np.dot(vector_a, vector_b)\n",
    "    norm_a = np.linalg.norm(vector_a)\n",
    "    norm_b = np.linalg.norm(vector_b)\n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Похожие слова: seminar - Косинусное сходство: 0.9147\n",
      "Похожие слова: courses - Косинусное сходство: 0.8138\n",
      "Слова из той же области: education - Косинусное сходство: 0.6976\n",
      "Совершенно другие слова: situation - Косинусное сходство: 0.6668\n",
      "Слова из той же области: laboratories - Косинусное сходство: 0.6318\n",
      "Совершенно другие слова: comments - Косинусное сходство: 0.6094\n",
      "Слова из той же области: conference - Косинусное сходство: 0.5666\n",
      "Совершенно другие слова: season - Косинусное сходство: 0.4208\n"
     ]
    }
   ],
   "source": [
    "similar_words = [\"seminar\", \"courses\"]\n",
    "domain_words = [\"education\", \"laboratories\", \"conference\"]\n",
    "different_words = [\"situation\", \"comments\", \"season\"]\n",
    "\n",
    "distances = calculate_distances('lecture', similar_words, domain_words, different_words, model, cosine_similarity_manual)\n",
    "\n",
    "for group, word, distance in distances:\n",
    "    print(f\"{group}: {word} - Косинусное сходство: {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_content(file_path):\n",
    "    filename = file_path.split('\\\\')[-1]\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "            return content, filename\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't read file {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "def vectorize_file(file_path):\n",
    "    content, filename = read_content(file_path)\n",
    "    vector = vectorize_text(content, model)\n",
    "    \n",
    "    vector_str = \"\\t\".join([str(component) for component in vector])\n",
    "    result_line = f\"{filename}\\t{vector_str}\"\n",
    "    \n",
    "    return result_line\n",
    "\n",
    "def vectorize_directory(directory_path):\n",
    "    all_lines = []\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            line = vectorize_file(file_path)\n",
    "            all_lines.append(line)\n",
    "            \n",
    "    tsv_filepath = os.path.join('..', 'assets', 'annotated-corpus', 'test.tsv')\n",
    "\n",
    "    os.makedirs(os.path.dirname(tsv_filepath), exist_ok=True)\n",
    "\n",
    "    with open(tsv_filepath, 'w') as f:\n",
    "        f.write(\"\\n\".join(all_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../../../../probe/dataset/20news-bydate-test\"\n",
    "\n",
    "vectorize_directory(dataset_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
